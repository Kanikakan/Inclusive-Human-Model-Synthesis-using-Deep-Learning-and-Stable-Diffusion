# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15TeHjX5rrxroYS2BPYCzWvUtVPOCSD80

## Load the dataset
"""

import pandas as pd

# Load the CSV file into a DataFrame
df = pd.read_csv('/content/ethinicitydataset - Sheet1.csv')

# Display the first few rows of the DataFrame
print(df.head())

# Loop through the rows of the DataFrame and access the image URL and corresponding prompt
for index, row in df.iterrows():
    image_url = row['Image']
    age = row['Age']
    ethinicity = row['Ethinicity']
    gender = row['Gender']
    outfit = row['Outfit']

    # Perform operations with image URL and prompt
    print(f"Image URL: {image_url}, Age: {age}, Ethinicity: {ethinicity}, Gender: {gender}, Outfit: {outfit}")

"""## Data Preprocessing"""

!pip install gdown

import pandas as pd
from PIL import Image
import os
import gdown

# Load your dataset CSV file
dataset_csv = '/content/ethinicitydataset - Sheet1.csv'
data = pd.read_csv(dataset_csv)

# Define the output directory for preprocessed images
output_dir = 'preprocessed_images'
os.makedirs(output_dir, exist_ok=True)

# Define the target size for resizing images
target_size = (256, 256)  # Adjust as needed

# Function to preprocess images
def preprocess_image(image_path, output_path):
    try:
        # Open the image
        image = Image.open(image_path)
        # Resize the image
        image = image.resize(target_size, Image.ANTIALIAS)
        # Convert to RGB if not already in RGB format
        if image.mode != 'RGB':
            image = image.convert('RGB')
        # Save the preprocessed image
        image.save(output_path)
        print(f"Image {image_path} preprocessed and saved to {output_path}")
    except Exception as e:
        print(f"Error processing image {image_path}: {e}")

# Iterate over rows in the dataset
for index, row in data.iterrows():
    image_id = row['Image'].split('/')[-2]
    image_path = f'/content/{image_id}.jpg'  # Output filename, you may need to adjust this
    age = row['Age']
    ethinicity = row['Ethinicity']
    gender = row['Gender']
    outfit = row['Outfit']
    output_path = os.path.join(output_dir, f"preprocessed_{index}.jpg")
    gdown.download(f'https://drive.google.com/uc?id={image_id}', image_path, quiet=False)
    preprocess_image(image_path, output_path)

import pandas as pd
from PIL import Image
import os
import requests
from io import BytesIO
from transformers import GPT2Tokenizer

# Initialize the tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Define the maximum sequence length for prompts
max_seq_length = 128  # Adjust as needed


# Function to preprocess prompts
def preprocess_age(age):
    # Tokenize the prompt
    tokens = tokenizer.encode(age, max_length=max_seq_length, truncation=True)
    # Pad or truncate tokens to the max sequence length
    padded_tokens = tokens + [tokenizer.pad_token_id] * (max_seq_length - len(tokens))
    return padded_tokens

# Function to preprocess prompts
def preprocess_ethnicity(ethinicity):
    # Tokenize the prompt
    tokens = tokenizer.encode(ethinicity, max_length=max_seq_length, truncation=True)
    # Pad or truncate tokens to the max sequence length
    padded_tokens = tokens + [tokenizer.pad_token_id] * (max_seq_length - len(tokens))
    return padded_tokens
def preprocess_gender(gender):
    # Tokenize the prompt
    tokens = tokenizer.encode(gender, max_length=max_seq_length, truncation=True)
    # Pad or truncate tokens to the max sequence length
    padded_tokens = tokens + [tokenizer.pad_token_id] * (max_seq_length - len(tokens))
    return padded_tokens
def preprocess_outfit(outfit):
    # Tokenize the prompt
    tokens = tokenizer.encode(outfit, max_length=max_seq_length, truncation=True)
    # Pad or truncate tokens to the max sequence length
    padded_tokens = tokens + [tokenizer.pad_token_id] * (max_seq_length - len(tokens))
    return padded_tokens

# Iterate over rows in the dataset
for index, row in data.iterrows():
    age = row['Age']
    ethnicity = row['Ethinicity']  # Replace 'Ethnicity' with the actual column name containing ethnicity
    gender = row['Gender']  # Replace 'Gender' with the actual column name containing gender
    outfit = row['Outfit']  # Replace 'Outfit' with the actual column name containing outfit

    # Preprocess each attribute


    processed_age = preprocess_age(age)
    processed_ethnicity = preprocess_ethnicity(ethinicity)  # Define a function preprocess_ethnicity
    processed_gender = preprocess_gender(gender)  # Define a function preprocess_gender
    processed_outfit = preprocess_outfit(outfit)  # Define a function preprocess_outfit
# Function to preprocess age



    print(f"Processed age: {processed_age}")
    print(f"Processed ethnicity: {processed_ethnicity}")
    print(f"Processed gender: {processed_gender}")
    print(f"Processed outfit: {processed_outfit}")

# Define directories to save the preprocessed attributes
age_dir = 'preprocessed_attributes/age'
ethnicity_dir = 'preprocessed_attributes/ethnicity'
gender_dir = 'preprocessed_attributes/gender'
outfit_dir = 'preprocessed_attributes/outfit'

# Create directories if they don't exist
os.makedirs(age_dir, exist_ok=True)
os.makedirs(ethnicity_dir, exist_ok=True)
os.makedirs(gender_dir, exist_ok=True)
os.makedirs(outfit_dir, exist_ok=True)

# Iterate over rows in the dataset
for index, row in data.iterrows():
    age = row['Age']
    ethnicity = row['Ethinicity']  # Replace 'Ethnicity' with the actual column name containing ethnicity
    gender = row['Gender']  # Replace 'Gender' with the actual column name containing gender
    outfit = row['Outfit']  # Replace 'Outfit' with the actual column name containing outfit

    # Preprocess each attribute
    processed_age = preprocess_age(age)
    processed_ethnicity = preprocess_ethnicity(ethnicity)
    processed_gender = preprocess_gender(gender)
    processed_outfit = preprocess_outfit(outfit)

    # Save the preprocessed attributes as text files
    with open(os.path.join(age_dir, f'{index}.txt'), 'w') as f:
        f.write(' '.join(map(str, processed_age)))

    with open(os.path.join(ethnicity_dir, f'{index}.txt'), 'w') as f:
        f.write(' '.join(map(str, processed_ethnicity)))

    with open(os.path.join(gender_dir, f'{index}.txt'), 'w') as f:
        f.write(' '.join(map(str, processed_gender)))

    with open(os.path.join(outfit_dir, f'{index}.txt'), 'w') as f:
        f.write(' '.join(map(str, processed_outfit)))

"""## Splitting datset"""

# Assuming you have a base directory for preprocessed age attributes
age_base_dir = '/content/preprocessed_attributes/age/'

# Generate paths for preprocessed age attributes from 0 to 8
preprocessed_age = [os.path.join(age_base_dir, f"{i}.txt") for i in range(49)]

print(preprocessed_age)

# Assuming you have a base directory for preprocessed age attributes
age_base_dir = '/content/preprocessed_attributes/outfit'

# Generate paths for preprocessed age attributes from 0 to 8
preprocessed_age = [os.path.join(age_base_dir, f"{i}.txt") for i in range(49)]

print(preprocessed_age)

preprocessed_image = ['/content/preprocessed_images/0.txt', '/content/preprocessed_images/1.txt', '/content/preprocessed_images/2.txt', '/content/preprocessed_images/3.txt', '/content/preprocessed_images/4.txt', '/content/preprocessed_images/5.txt', '/content/preprocessed_images/6.txt', '/content/preprocessed_images/7.txt', '/content/preprocessed_images/8.txt', '/content/preprocessed_images/9.txt', '/content/preprocessed_images/10.txt', '/content/preprocessed_images/11.txt', '/content/preprocessed_images/12.txt', '/content/preprocessed_images/13.txt', '/content/preprocessed_images/14.txt', '/content/preprocessed_images/15.txt', '/content/preprocessed_images/16.txt', '/content/preprocessed_images/17.txt', '/content/preprocessed_images/18.txt', '/content/preprocessed_images/19.txt', '/content/preprocessed_images/20.txt', '/content/preprocessed_images/21.txt', '/content/preprocessed_images/22.txt', '/content/preprocessed_images/23.txt', '/content/preprocessed_images/24.txt', '/content/preprocessed_images/25.txt', '/content/preprocessed_images/26.txt', '/content/preprocessed_images/27.txt', '/content/preprocessed_images/28.txt', '/content/preprocessed_images/29.txt', '/content/preprocessed_images/30.txt', '/content/preprocessed_images/31.txt', '/content/preprocessed_images/32.txt', '/content/preprocessed_images/33.txt', '/content/preprocessed_images/34.txt', '/content/preprocessed_images/35.txt', '/content/preprocessed_images/36.txt', '/content/preprocessed_images/37.txt', '/content/preprocessed_images/38.txt', '/content/preprocessed_images/39.txt', '/content/preprocessed_images/40.txt', '/content/preprocessed_images/41.txt', '/content/preprocessed_images/42.txt', '/content/preprocessed_images/43.txt', '/content/preprocessed_images/44.txt', '/content/preprocessed_images/45.txt', '/content/preprocessed_images/46.txt', '/content/preprocessed_images/47.txt', '/content/preprocessed_images/48.txt']
preprocessed_age = ['/content/preprocessed_attributes/age/0.txt', '/content/preprocessed_attributes/age/1.txt', '/content/preprocessed_attributes/age/2.txt', '/content/preprocessed_attributes/age/3.txt', '/content/preprocessed_attributes/age/4.txt', '/content/preprocessed_attributes/age/5.txt', '/content/preprocessed_attributes/age/6.txt', '/content/preprocessed_attributes/age/7.txt', '/content/preprocessed_attributes/age/8.txt', '/content/preprocessed_attributes/age/9.txt', '/content/preprocessed_attributes/age/10.txt', '/content/preprocessed_attributes/age/11.txt', '/content/preprocessed_attributes/age/12.txt', '/content/preprocessed_attributes/age/13.txt', '/content/preprocessed_attributes/age/14.txt', '/content/preprocessed_attributes/age/15.txt', '/content/preprocessed_attributes/age/16.txt', '/content/preprocessed_attributes/age/17.txt', '/content/preprocessed_attributes/age/18.txt', '/content/preprocessed_attributes/age/19.txt', '/content/preprocessed_attributes/age/20.txt', '/content/preprocessed_attributes/age/21.txt', '/content/preprocessed_attributes/age/22.txt', '/content/preprocessed_attributes/age/23.txt', '/content/preprocessed_attributes/age/24.txt', '/content/preprocessed_attributes/age/25.txt', '/content/preprocessed_attributes/age/26.txt', '/content/preprocessed_attributes/age/27.txt', '/content/preprocessed_attributes/age/28.txt', '/content/preprocessed_attributes/age/29.txt', '/content/preprocessed_attributes/age/30.txt', '/content/preprocessed_attributes/age/31.txt', '/content/preprocessed_attributes/age/32.txt', '/content/preprocessed_attributes/age/33.txt', '/content/preprocessed_attributes/age/34.txt', '/content/preprocessed_attributes/age/35.txt', '/content/preprocessed_attributes/age/36.txt', '/content/preprocessed_attributes/age/37.txt', '/content/preprocessed_attributes/age/38.txt', '/content/preprocessed_attributes/age/39.txt', '/content/preprocessed_attributes/age/40.txt', '/content/preprocessed_attributes/age/41.txt', '/content/preprocessed_attributes/age/42.txt', '/content/preprocessed_attributes/age/43.txt', '/content/preprocessed_attributes/age/44.txt', '/content/preprocessed_attributes/age/45.txt', '/content/preprocessed_attributes/age/46.txt', '/content/preprocessed_attributes/age/47.txt', '/content/preprocessed_attributes/age/48.txt']
preprocessed_ethnicity = ['/content/preprocessed_attributes/ethnicity/0.txt', '/content/preprocessed_attributes/ethnicity/1.txt', '/content/preprocessed_attributes/ethnicity/2.txt', '/content/preprocessed_attributes/ethnicity/3.txt', '/content/preprocessed_attributes/ethnicity/4.txt', '/content/preprocessed_attributes/ethnicity/5.txt', '/content/preprocessed_attributes/ethnicity/6.txt', '/content/preprocessed_attributes/ethnicity/7.txt', '/content/preprocessed_attributes/ethnicity/8.txt', '/content/preprocessed_attributes/ethnicity/9.txt', '/content/preprocessed_attributes/ethnicity/10.txt', '/content/preprocessed_attributes/ethnicity/11.txt', '/content/preprocessed_attributes/ethnicity/12.txt', '/content/preprocessed_attributes/ethnicity/13.txt', '/content/preprocessed_attributes/ethnicity/14.txt', '/content/preprocessed_attributes/ethnicity/15.txt', '/content/preprocessed_attributes/ethnicity/16.txt', '/content/preprocessed_attributes/ethnicity/17.txt', '/content/preprocessed_attributes/ethnicity/18.txt', '/content/preprocessed_attributes/ethnicity/19.txt', '/content/preprocessed_attributes/ethnicity/20.txt', '/content/preprocessed_attributes/ethnicity/21.txt', '/content/preprocessed_attributes/ethnicity/22.txt', '/content/preprocessed_attributes/ethnicity/23.txt', '/content/preprocessed_attributes/ethnicity/24.txt', '/content/preprocessed_attributes/ethnicity/25.txt', '/content/preprocessed_attributes/ethnicity/26.txt', '/content/preprocessed_attributes/ethnicity/27.txt', '/content/preprocessed_attributes/ethnicity/28.txt', '/content/preprocessed_attributes/ethnicity/29.txt', '/content/preprocessed_attributes/ethnicity/30.txt', '/content/preprocessed_attributes/ethnicity/31.txt', '/content/preprocessed_attributes/ethnicity/32.txt', '/content/preprocessed_attributes/ethnicity/33.txt', '/content/preprocessed_attributes/ethnicity/34.txt', '/content/preprocessed_attributes/ethnicity/35.txt', '/content/preprocessed_attributes/ethnicity/36.txt', '/content/preprocessed_attributes/ethnicity/37.txt', '/content/preprocessed_attributes/ethnicity/38.txt', '/content/preprocessed_attributes/ethnicity/39.txt', '/content/preprocessed_attributes/ethnicity/40.txt', '/content/preprocessed_attributes/ethnicity/41.txt', '/content/preprocessed_attributes/ethnicity/42.txt', '/content/preprocessed_attributes/ethnicity/43.txt', '/content/preprocessed_attributes/ethnicity/44.txt', '/content/preprocessed_attributes/ethnicity/45.txt', '/content/preprocessed_attributes/ethnicity/46.txt', '/content/preprocessed_attributes/ethnicity/47.txt', '/content/preprocessed_attributes/ethnicity/48.txt']
preprocessed_gender = ['/content/preprocessed_attributes/gender/0.txt', '/content/preprocessed_attributes/gender/1.txt', '/content/preprocessed_attributes/gender/2.txt', '/content/preprocessed_attributes/gender/3.txt', '/content/preprocessed_attributes/gender/4.txt', '/content/preprocessed_attributes/gender/5.txt', '/content/preprocessed_attributes/gender/6.txt', '/content/preprocessed_attributes/gender/7.txt', '/content/preprocessed_attributes/gender/8.txt', '/content/preprocessed_attributes/gender/9.txt', '/content/preprocessed_attributes/gender/10.txt', '/content/preprocessed_attributes/gender/11.txt', '/content/preprocessed_attributes/gender/12.txt', '/content/preprocessed_attributes/gender/13.txt', '/content/preprocessed_attributes/gender/14.txt', '/content/preprocessed_attributes/gender/15.txt', '/content/preprocessed_attributes/gender/16.txt', '/content/preprocessed_attributes/gender/17.txt', '/content/preprocessed_attributes/gender/18.txt', '/content/preprocessed_attributes/gender/19.txt', '/content/preprocessed_attributes/gender/20.txt', '/content/preprocessed_attributes/gender/21.txt', '/content/preprocessed_attributes/gender/22.txt', '/content/preprocessed_attributes/gender/23.txt', '/content/preprocessed_attributes/gender/24.txt', '/content/preprocessed_attributes/gender/25.txt', '/content/preprocessed_attributes/gender/26.txt', '/content/preprocessed_attributes/gender/27.txt', '/content/preprocessed_attributes/gender/28.txt', '/content/preprocessed_attributes/gender/29.txt', '/content/preprocessed_attributes/gender/30.txt', '/content/preprocessed_attributes/gender/31.txt', '/content/preprocessed_attributes/gender/32.txt', '/content/preprocessed_attributes/gender/33.txt', '/content/preprocessed_attributes/gender/34.txt', '/content/preprocessed_attributes/gender/35.txt', '/content/preprocessed_attributes/gender/36.txt', '/content/preprocessed_attributes/gender/37.txt', '/content/preprocessed_attributes/gender/38.txt', '/content/preprocessed_attributes/gender/39.txt', '/content/preprocessed_attributes/gender/40.txt', '/content/preprocessed_attributes/gender/41.txt', '/content/preprocessed_attributes/gender/42.txt', '/content/preprocessed_attributes/gender/43.txt', '/content/preprocessed_attributes/gender/44.txt', '/content/preprocessed_attributes/gender/45.txt', '/content/preprocessed_attributes/gender/46.txt', '/content/preprocessed_attributes/gender/47.txt', '/content/preprocessed_attributes/gender/48.txt']
preprocessed_outfit = ['/content/preprocessed_attributes/outfit/0.txt', '/content/preprocessed_attributes/outfit/1.txt', '/content/preprocessed_attributes/outfit/2.txt', '/content/preprocessed_attributes/outfit/3.txt', '/content/preprocessed_attributes/outfit/4.txt', '/content/preprocessed_attributes/outfit/5.txt', '/content/preprocessed_attributes/outfit/6.txt', '/content/preprocessed_attributes/outfit/7.txt', '/content/preprocessed_attributes/outfit/8.txt', '/content/preprocessed_attributes/outfit/9.txt', '/content/preprocessed_attributes/outfit/10.txt', '/content/preprocessed_attributes/outfit/11.txt', '/content/preprocessed_attributes/outfit/12.txt', '/content/preprocessed_attributes/outfit/13.txt', '/content/preprocessed_attributes/outfit/14.txt', '/content/preprocessed_attributes/outfit/15.txt', '/content/preprocessed_attributes/outfit/16.txt', '/content/preprocessed_attributes/outfit/17.txt', '/content/preprocessed_attributes/outfit/18.txt', '/content/preprocessed_attributes/outfit/19.txt', '/content/preprocessed_attributes/outfit/20.txt', '/content/preprocessed_attributes/outfit/21.txt', '/content/preprocessed_attributes/outfit/22.txt', '/content/preprocessed_attributes/outfit/23.txt', '/content/preprocessed_attributes/outfit/24.txt', '/content/preprocessed_attributes/outfit/25.txt', '/content/preprocessed_attributes/outfit/26.txt', '/content/preprocessed_attributes/outfit/27.txt', '/content/preprocessed_attributes/outfit/28.txt', '/content/preprocessed_attributes/outfit/29.txt', '/content/preprocessed_attributes/outfit/30.txt', '/content/preprocessed_attributes/outfit/31.txt', '/content/preprocessed_attributes/outfit/32.txt', '/content/preprocessed_attributes/outfit/33.txt', '/content/preprocessed_attributes/outfit/34.txt', '/content/preprocessed_attributes/outfit/35.txt', '/content/preprocessed_attributes/outfit/36.txt', '/content/preprocessed_attributes/outfit/37.txt', '/content/preprocessed_attributes/outfit/38.txt', '/content/preprocessed_attributes/outfit/39.txt', '/content/preprocessed_attributes/outfit/40.txt', '/content/preprocessed_attributes/outfit/41.txt', '/content/preprocessed_attributes/outfit/42.txt', '/content/preprocessed_attributes/outfit/43.txt', '/content/preprocessed_attributes/outfit/44.txt', '/content/preprocessed_attributes/outfit/45.txt', '/content/preprocessed_attributes/outfit/46.txt', '/content/preprocessed_attributes/outfit/47.txt', '/content/preprocessed_attributes/outfit/48.txt']

import pandas as pd
import os

from sklearn.model_selection import train_test_split

# Define the path for saving the preprocessed dataset
output_dir = '/content/preprocessed_dataset'


# Split the dataset into training, validation, and testing sets
train_data_img, test_data_img, train_data_age, test_data_age,train_data_ethnicity, test_data_ethnicity,train_data_gender, test_data_gender, train_data_outfit, test_data_outfit  = train_test_split(preprocessed_image, preprocessed_age, preprocessed_ethnicity, preprocessed_gender, preprocessed_outfit, test_size=0.2, random_state=42)
train_data_img, val_data_img, train_data_age, val_data_age, train_data_ethnicity, val_data_ethnicity, train_data_gender, val_data_gender, train_data_outfit, val_data_outfit = train_test_split(train_data_img, train_data_age, train_data_ethnicity, train_data_gender, train_data_outfit, test_size=0.1, random_state=42)


# Convert the lists to DataFrames
train_data_img = pd.DataFrame(train_data_img, columns=['Image'])
val_data_img = pd.DataFrame(val_data_img, columns=['Image'])
test_data_img = pd.DataFrame(test_data_img, columns=['Image'])

train_data_age = pd.DataFrame(train_data_age, columns=['Age'])
val_data_age = pd.DataFrame(val_data_age, columns=['Age'])
test_data_age = pd.DataFrame(test_data_age, columns=['Age'])

train_data_ethnicity = pd.DataFrame(train_data_ethnicity, columns=['Ethnicity'])
val_data_ethnicity = pd.DataFrame(val_data_ethnicity, columns=['Ethnicity'])
test_data_ethnicity = pd.DataFrame(test_data_ethnicity, columns=['Ethnicity'])

train_data_gender = pd.DataFrame(train_data_gender, columns=['Gender'])
val_data_gender = pd.DataFrame(val_data_gender, columns=['Gender'])
test_data_gender = pd.DataFrame(test_data_gender, columns=['Gender'])

train_data_outfit = pd.DataFrame(train_data_outfit, columns=['Outfit'])
val_data_outfit = pd.DataFrame(val_data_outfit, columns=['Outfit'])
test_data_outfit = pd.DataFrame(test_data_outfit, columns=['Outfit'])


# Shuffle the datasets
train_data_img = train_data_img.sample(frac=1, random_state=42).reset_index(drop=True)
val_data_img = val_data_img.sample(frac=1, random_state=42).reset_index(drop=True)
test_data_img = test_data_img.sample(frac=1, random_state=42).reset_index(drop=True)

# Shuffling data for age column
train_data_age = train_data_age.sample(frac=1, random_state=42).reset_index(drop=True)
val_data_age = val_data_age.sample(frac=1, random_state=42).reset_index(drop=True)
test_data_age = test_data_age.sample(frac=1, random_state=42).reset_index(drop=True)

# Shuffling data for ethnicity column
train_data_ethnicity = train_data_ethnicity.sample(frac=1, random_state=42).reset_index(drop=True)
val_data_ethnicity = val_data_ethnicity.sample(frac=1, random_state=42).reset_index(drop=True)
test_data_ethnicity = test_data_ethnicity.sample(frac=1, random_state=42).reset_index(drop=True)

# Shuffling data for gender column
train_data_gender = train_data_gender.sample(frac=1, random_state=42).reset_index(drop=True)
val_data_gender = val_data_gender.sample(frac=1, random_state=42).reset_index(drop=True)
test_data_gender = test_data_gender.sample(frac=1, random_state=42).reset_index(drop=True)

# Shuffling data for outfit column
train_data_outfit = train_data_outfit.sample(frac=1, random_state=42).reset_index(drop=True)
val_data_outfit = val_data_outfit.sample(frac=1, random_state=42).reset_index(drop=True)
test_data_outfit = test_data_outfit.sample(frac=1, random_state=42).reset_index(drop=True)

# Save the preprocessed dataset
os.makedirs(output_dir, exist_ok=True)
train_data_img.to_csv(os.path.join(output_dir, 'train_images.csv'), index=False)
val_data_img.to_csv(os.path.join(output_dir, 'val_images.csv'), index=False)
test_data_img.to_csv(os.path.join(output_dir, 'test_images.csv'), index=False)

# Assuming train_data_age, train_data_ethnicity, train_data_gender, and train_data_outfit
# contain the dataframes for age, ethnicity, gender, and outfit respectively
train_data_age.to_csv(os.path.join(output_dir, 'train_age.csv'), index=False)
train_data_ethnicity.to_csv(os.path.join(output_dir, 'train_ethnicity.csv'), index=False)
train_data_gender.to_csv(os.path.join(output_dir, 'train_gender.csv'), index=False)
train_data_outfit.to_csv(os.path.join(output_dir, 'train_outfit.csv'), index=False)

# Similarly for validation data
val_data_age.to_csv(os.path.join(output_dir, 'val_age.csv'), index=False)
val_data_ethnicity.to_csv(os.path.join(output_dir, 'val_ethnicity.csv'), index=False)
val_data_gender.to_csv(os.path.join(output_dir, 'val_gender.csv'), index=False)
val_data_outfit.to_csv(os.path.join(output_dir, 'val_outfit.csv'), index=False)

# Similarly for test data
test_data_age.to_csv(os.path.join(output_dir, 'test_age.csv'), index=False)
test_data_ethnicity.to_csv(os.path.join(output_dir, 'test_ethnicity.csv'), index=False)
test_data_gender.to_csv(os.path.join(output_dir, 'test_gender.csv'), index=False)
test_data_outfit.to_csv(os.path.join(output_dir, 'test_outfit.csv'), index=False)


print("Dataset split and saved successfully.")

"""## Training"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
import pandas as pd
from PIL import Image
import os

from diffusers import AutoPipelineForImage2Image
from diffusers.utils import load_image, make_image_grid

# Define the custom dataset class
class CustomDataset(Dataset):
    def __init__(self, csv_file, image_dir, transform=None):
        self.data = pd.read_csv(csv_file)
        self.image_dir = image_dir
        self.transform = transform

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img_name = os.path.join(self.image_dir, self.data.iloc[idx, 0])
        image = Image.open(img_name)
        if self.transform:
            image = self.transform(image)
        return image

# Define data transformations for the custom dataset
transform = transforms.Compose([
    transforms.Resize((256, 256)),
    transforms.ToTensor(),
])

# Define paths to preprocessed dataset CSV files
train_csv_dir = '/content/preprocessed_dataset/train_images.csv'
val_csv_dir = '/content/preprocessed_dataset/val_images.csv'
test_csv_dir = '/content/preprocessed_dataset/test_images.csv'
image_dir = '/path/to/images/directory'  # Replace with the actual path to your images directory

# Create training, validation, and test datasets
train_dataset = CustomDataset(csv_file=train_csv_dir, image_dir=image_dir, transform=transform)
val_dataset = CustomDataset(csv_file=val_csv_dir, image_dir=image_dir, transform=transform)
test_dataset = CustomDataset(csv_file=test_csv_dir, image_dir=image_dir, transform=transform)

# Create data loaders
batch_size = 32  # Set your desired batch size
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Initialize the AutoPipelineForImage2Image model
pipeline = AutoPipelineForImage2Image.from_pretrained(
    "SG161222/RealVisXL_V3.0_Turbo", torch_dtype=torch.float16, use_safetensors=True
)
pipeline.enable_model_cpu_offload()
pipeline.enable_xformers_memory_efficient_attention()

# Define loss function and optimizer
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(pipeline.parameters(), lr=learning_rate)

# Train the model
for epoch in range(num_epochs):
    pipeline.train()
    running_loss = 0.0
    for images in train_loader:
        optimizer.zero_grad()
        outputs = pipeline(images)
        loss = criterion(outputs, targets)  # Define targets according to your task
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * images.size(0)
    epoch_loss = running_loss / len(train_dataset)
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss}")

# Save the trained model
torch.save(pipeline.state_dict(), 'auto_pipeline_model.pth')

!pip uninstall torch torchvision
!pip install torch torchvision

from diffusers import UNet2DModel

model = UNet2DModel(
    sample_size=config.image_size,  # the target image resolution
    in_channels=3,  # the number of input channels, 3 for RGB images
    out_channels=3,  # the number of output channels
    layers_per_block=2,  # how many ResNet layers to use per UNet block
    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channels for each UNet block
    down_block_types=(
        "DownBlock2D",  # a regular ResNet downsampling block
        "DownBlock2D",
        "DownBlock2D",
        "DownBlock2D",
        "AttnDownBlock2D",  # a ResNet downsampling block with spatial self-attention
        "DownBlock2D",
    ),
    up_block_types=(
        "UpBlock2D",  # a regular ResNet upsampling block
        "AttnUpBlock2D",  # a ResNet upsampling block with spatial self-attention
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
    ),
)

from diffusers.optimization import get_cosine_schedule_with_warmup

optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)
lr_scheduler = get_cosine_schedule_with_warmup(
    optimizer=optimizer,
    num_warmup_steps=config.lr_warmup_steps,
    num_training_steps=(len(train_dataloader) * config.num_epochs),
)

from diffusers import DDPMPipeline
from diffusers.utils import make_image_grid
import os

def evaluate(config, epoch, pipeline):
    # Sample some images from random noise (this is the backward diffusion process).
    # The default pipeline output type is `List[PIL.Image]`
    images = pipeline(
        batch_size=config.eval_batch_size,
        generator=torch.manual_seed(config.seed),
    ).images

    # Make a grid out of the images
    image_grid = make_image_grid(images, rows=4, cols=4)

    # Save the images
    test_dir = os.path.join(config.output_dir, "samples")
    os.makedirs(test_dir, exist_ok=True)
    image_grid.save(f"{test_dir}/{epoch:04d}.png")

from accelerate import Accelerator
from huggingface_hub import create_repo, upload_folder
from tqdm.auto import tqdm
from pathlib import Path
import os

def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):
    # Initialize accelerator and tensorboard logging
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        log_with="tensorboard",
        project_dir=os.path.join(config.output_dir, "logs"),
    )
    if accelerator.is_main_process:
        if config.output_dir is not None:
            os.makedirs(config.output_dir, exist_ok=True)
        if config.push_to_hub:
            repo_id = create_repo(
                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True
            ).repo_id
        accelerator.init_trackers("train_example")

    # Prepare everything
    # There is no specific order to remember, you just need to unpack the
    # objects in the same order you gave them to the prepare method.
    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, lr_scheduler
    )

    global_step = 0

    # Now you train the model
    for epoch in range(config.num_epochs):
        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)
        progress_bar.set_description(f"Epoch {epoch}")

        for step, batch in enumerate(train_dataloader):
            clean_images = batch["images"]
            # Sample noise to add to the images
            noise = torch.randn(clean_images.shape, device=clean_images.device)
            bs = clean_images.shape[0]

            # Sample a random timestep for each image
            timesteps = torch.randint(
                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,
                dtype=torch.int64
            )

            # Add noise to the clean images according to the noise magnitude at each timestep
            # (this is the forward diffusion process)
            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)

            with accelerator.accumulate(model):
                # Predict the noise residual
                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]
                loss = F.mse_loss(noise_pred, noise)
                accelerator.backward(loss)

                accelerator.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            progress_bar.update(1)
            logs = {"loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0], "step": global_step}
            progress_bar.set_postfix(**logs)
            accelerator.log(logs, step=global_step)
            global_step += 1

        # After each epoch you optionally sample some demo images with evaluate() and save the model
        if accelerator.is_main_process:
            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)

            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:
                evaluate(config, epoch, pipeline)

            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:
                if config.push_to_hub:
                    upload_folder(
                        repo_id=repo_id,
                        folder_path=config.output_dir,
                        commit_message=f"Epoch {epoch}",
                        ignore_patterns=["step_*", "epoch_*"],
                    )
                else:
                    pipeline.save_pretrained(config.output_dir)

!pip install diffusers transformers ftfy accelerate
!pip install xformers

import torch
from diffusers import AutoPipelineForImage2Image
from diffusers.utils import load_image, make_image_grid
import torch



# Now you can use the loaded_model for inference or further processing


pipeline = AutoPipelineForImage2Image.from_pretrained(
    "SG161222/RealVisXL_V3.0_Turbo", torch_dtype=torch.float16, use_safetensors=True
)
pipeline.enable_model_cpu_offload()
# remove following line if xFormers is not installed or you have PyTorch 2.0 or higher installed
pipeline.enable_xformers_memory_efficient_attention()

init_image = load_image("/content/jstroar.jpg")
prompt = ["Australian bold male, aged 25 wearing t-shirt wriiten 'JUST ROAR'"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

# Save the generated image
image.save("generated_image.png")

init_image = load_image("/content/WhatsApp Image 2024-03-19 at 8.29.51 PM.jpeg")
prompt = ["Australian bold male, aged 25 wearing blazer with buttons"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/WhatsApp Image 2024-03-19 at 8.29.51 PM.jpeg")
prompt = ["Chinese bold male, aged 25 wearing blazer"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/WhatsApp Image 2024-03-19 at 8.29.51 PM.jpeg")
prompt = ["African bold male, aged 25 wearing blazer "]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/WhatsApp Image 2024-03-19 at 8.37.49 PM.jpeg")
prompt = ["African bold male, aged 25 wearing blazer "]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)





!pip install gradio

import gradio as gr
from diffusers import DiffusionPipeline
from diffusers.utils import load_image


# Prediction Method
def predict(image, prompt):
    # Load the image
    init_image = load_image(image)

    # Define negative prompt
    neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)

    # Pass prompt and image to pipeline United Colors of Benetton
    generated_image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]

    return generated_image

# Gradio run and implement interface
gr.Interface(
    predict,
    title='Inclusive Human Synthesis',
    inputs=[
        gr.inputs.Image(source='upload', type='pil'),
        gr.inputs.Textbox(label='prompt')
    ],
    outputs=[
        gr.outputs.Image()
    ]
).launch(debug=True, share=True)

init_image = load_image("/content/ranveer.webp")
prompt = ["Chinese bold male, aged 25 wearing white t-shirt"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/ranveer.webp")
prompt = ["Australian bold male, aged 25 wearing white shirt"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/ranveer2.jpg")
prompt = ["Australian bold male, aged 25 wearing printed shirt"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/ranveer2.jpg")
prompt = ["Chinese bold male, aged 25 wearing printed shirt"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/ranbir.jpg")
prompt = ["Chinese bold male, aged 25 wearing shirt, with voluminous, curly hair styled in a side part."]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/ranbir.jpg")
prompt = ["Chinese bold male, aged 25 wearing shirt,  with ombre hair (gradually lightening color) in a Short bowl cut with blunt fringe"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/ranbir.jpg")
prompt = ["Australian bold male, aged 25 wearing shirt,with a short sides and a quiff styled with a relaxed wave."]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/ranbir.jpg")
prompt = ["Australian bold male, aged 25 wearing shirt, with a short 'Aussie mullet' hairstyle, styled with a matte product for texture"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/ranbir.jpg")
prompt = ["African bold male, aged 25 wearing shirt, with a natural afro hairstyle, accessorized with a beaded headband."]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/ranbir2.jpg")
prompt = ["Australian bold male, aged 25 wearing shirt, with long, straight black hair tied back in a low ponytail"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/ranbir2.jpg")
prompt = ["Chinese bold male, aged 25 wearing shirt, with medium-length messy hair, textured with product"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)











init_image = load_image("/content/floral white.webp")
prompt = ["Indian bold female, aged 25 wearing floral shirt"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/floral white.webp")
prompt = ["Australian bold female, aged 25 wearing floral shirt"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/floral white.webp")
prompt = ["African bold female, aged 25 wearing floral shirt"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/babe.webp")
prompt = ["Chinese bold female, aged 25 wearing shirt with 'BABE' written on"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/whiteshirt.webp")
prompt = ["African bold female, aged 25 wearing shirt"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/whiteshirt.webp")
prompt = ["Australian bold female, aged 25 wearing shirt"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/whiteshirt.webp")
prompt = ["Indian bold female, aged 25 wearing shirt"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/babe.webp")
prompt = ["Indian bold female, aged 25 wearing shirt with 'BABE' written on"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/babe.webp")
prompt = ["Australian bold female, aged 50 wearing shirt with 'BABE' written on"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/babe.webp")
prompt = ["Australian bold female, aged 25 wearing shirt with 'BABE' written on"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/babe.webp")
prompt = ["African bold female, aged 25 wearing shirt with 'BABE' written on"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/babe.webp")
prompt = ["Chinese bold female, aged 25 wearing shirt with 'BABE' written on"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/babe.webp")
prompt = ["white american bold female, aged 25 wearing shirt with 'BABE' written on"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/babe.webp")
prompt = ["Turkish bold female, aged 25 wearing shirt with 'BABE' written on"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/man.jpeg")
prompt = ["An American face male model wearing white shirt, black pants with red tie"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch"] * len(prompt)

# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/man.jpeg")
prompt = ["An African black face male model wearing white shirt, black pants with red tie"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch"] * len(prompt)

# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/man.jpeg")
prompt = ["An Chinese face male model wearing white shirt, black pants with red tie"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch"] * len(prompt)

# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/man.jpeg")
prompt = ["An Turkish face male model wearing white shirt, black pants with red tie"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch"] * len(prompt)

# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/black floral.webp")
prompt = ["Chinese bold female, aged 25 wearing black printed shirt with proper eyes"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/black floral.webp")
prompt = ["American bold female, aged 25 wearing black printed shirt"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/black floral.webp")
prompt = ["African bold female, aged 25 wearing black printed shirt"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/black floral.webp")
prompt = ["Indian bold female, aged 25 wearing black printed shirt"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/women.jpeg")
prompt = ["An African black face female model wearing white pants and white blazers"]

neg_prompt = ["worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch"] * len(prompt)

# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/women.jpeg")
prompt = ["An Chinese face female model wearing white pants and white blazers"]

neg_prompt = ["worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch"] * len(prompt)

# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/women.jpeg")
prompt = ["An Turkish face female model wearing white pants and white blazers"]

neg_prompt = ["worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch"] * len(prompt)

# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/Screenshot 2024-03-13 134800.png")
prompt = ["Chinese woman of 30 years"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)
# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/Screenshot 2024-03-14 092351.png")
prompt = ["American woman of 30 years"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)
# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/Screenshot 2024-03-13 134800.png")
prompt = ["Chinese woman of 30 years"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)
# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/Screenshot 2024-03-13 134800.png")
prompt = ["Australian woman of 50 years"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)
# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/Screenshot 2024-03-13 134800.png")
prompt = ["African woman of 25 years"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)
# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/shirt and shorts.jpg")
prompt = ["white American bold female, aged 25 wearing printed top and shorts "]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)



init_image = load_image("/shirt and shorts.jpg")
prompt = ["Australian bold female, aged 25 wearing printed top and shorts "]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/shirt and shorts.jpg")
prompt = ["Indian bold female, aged 25 wearing printed top and shorts "]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/shirt and shorts.jpg")
prompt = ["Chinese bold female, aged 25 wearing printed top and shorts "]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/shirt and shorts.jpg")
prompt = ["African bold female, aged 25 wearing printed top and shorts "]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/ck blue.jpg")

prompt = ["Chinese bold male aged 25 wearing Calvin Klein t-shirt with logo 'Calvin Klein'"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/ck blue.jpg")

prompt = ["Indian bold male aged 25 wearing Calvin Klein t-shirt with logo 'Calvin Klein'"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/ck blue.jpg")

prompt = ["American bold male aged 25 wearing Calvin Klein t-shirt with logo 'Calvin Klein'"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/ck blue.jpg")

prompt = ["African bold male aged 25 wearing Calvin Klein t-shirt with logo 'Calvin Klein'"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)



init_image = load_image("/content/bb.jpg")
prompt = ["An African female model, young aged, smiling face, no change in cloth,no change in apparels, no change in dress,proper eyes, high quality image, opend eyes"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/bb.jpg")
prompt = ["An korean female model, young aged, smiling face, no change in cloth,no change in apparels, no change in dress,proper eyes, high quality image, opend eyes"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/bb.jpg")
prompt = ["An Australian female model, young aged, smiling face, no change in cloth,no change in apparels, no change in dress,proper eyes, high quality image, opend eyes"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/bb.jpg")
prompt = ["white american female model, 55 years aged, smiling face, no change in cloth,no change in apparels, no change in dress,proper eyes, high quality image, opend eyes"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/bb.jpg")
prompt = ["Turkish female model, 40 aged, smiling face, no change in cloth,no change in apparels, no change in dress,proper eyes, high quality image, opend eyes"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/bb.jpg")
prompt = ["Indian female model, 25 aged, without any spot in face"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/floral.jpg")
prompt = ["African bold female, aged 25 wearing floral top and shorts "]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/jstroar.jpg")
prompt = ["Australian bold male, aged 25 wearing t-shirt wriiten 'JUST ROAR'"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/floral.jpg")
prompt = ["Chinese bold female, aged 25 wearing floral top and shorts"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/floral.jpg")
prompt = ["Indian bold female, aged 25 wearing floral printed top and shorts"]
neg_prompt = ["worst quality,ugly face, closed eyes, disfigured arms low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline United Colors of Benetton
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/as b.jpg")
prompt = ["Indian confident male aged 30 wearing Adidas tshirt with logo and high quality image, opend eyes"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/as b.jpg")
prompt = ["Turkish confident male aged 30 wearing Adidas tshirt with logo and high quality image, opend eyes"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)


# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)



init_image = load_image("/content/WhatsApp Image 2024-03-13 at 12.55.40 PM.jpeg")
prompt = ["Australian woman of 30 years"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)
# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/WhatsApp Image 2024-03-13 at 12.55.40 PM.jpeg")
prompt = ["American woman of 30 years"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)
# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

init_image = load_image("/content/WhatsApp Image 2024-03-13 at 12.55.40 PM.jpeg")
prompt = ["Chinese woman of 30 years"]
neg_prompt = ["worst quality,ugly face, closed eyes, low quality, illustration, 3d, 2d, painting, cartoons, sketch, Provocative narrative"] * len(prompt)
# pass prompt and image to pipeline
image = pipeline(prompt, negative_prompt=neg_prompt, image=init_image).images[0]
make_image_grid([init_image, image], rows=1, cols=2)

"""### **IMAGE QUALITY CHECK METRICS**"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

def calculate_image_quality(image):
    # Convert image to grayscale
    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

    # Convert grayscale image to 8-bit depth
    gray_image = cv2.convertScaleAbs(gray_image)

    # Calculate image sharpness using Laplacian operator
    sharpness = cv2.Laplacian(gray_image, cv2.CV_64F).var()

    # Calculate image brightness
    brightness = np.mean(gray_image)

    # Calculate image saturation
    hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)
    saturation = np.mean(hsv_image[:,:,1])

    return sharpness, brightness, saturation

def visualize_image_quality_comparison(input_image, generated_image, input_metrics, generated_metrics):
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))

    axes[0, 0].imshow(input_image)
    axes[0, 0].set_title('Input Image')
    axes[0, 0].axis('off')

    axes[0, 1].imshow(generated_image)
    axes[0, 1].set_title('Generated Image')
    axes[0, 1].axis('off')

    axes[1, 0].bar(['Sharpness', 'Brightness', 'Saturation'], input_metrics, color='blue', alpha=0.6)
    axes[1, 0].set_title('Input Image Metrics')

    axes[1, 1].bar(['Sharpness', 'Brightness', 'Saturation'], generated_metrics, color='orange', alpha=0.6)
    axes[1, 1].set_title('Generated Image Metrics')

    plt.tight_layout()
    plt.show()

# Load input and generated images
input_image_path = "/content/women.jpeg"
generated_image_path = "/content/generated_image.jpg"

input_image = cv2.imread(input_image_path)
input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)

generated_image = cv2.imread(generated_image_path)
generated_image = cv2.cvtColor(generated_image, cv2.COLOR_BGR2RGB)

# Calculate image quality metrics
input_metrics = calculate_image_quality(input_image)
generated_metrics = calculate_image_quality(generated_image)

# Visualize image quality comparison
visualize_image_quality_comparison(input_image, generated_image, input_metrics, generated_metrics)

# Print quality metrics
print("Input Image Metrics:")
print("Sharpness:", input_metrics[0])
print("Brightness:", input_metrics[1])
print("Saturation:", input_metrics[2])

print("\nGenerated Image Metrics:")
print("Sharpness:", generated_metrics[0])
print("Brightness:", generated_metrics[1])
print("Saturation:", generated_metrics[2])

import matplotlib.pyplot as plt

def visualize_quality_comparison(input_metrics, generated_metrics):
    metrics_labels = ['Sharpness', 'Brightness', 'Saturation']
    bar_width = 0.35
    index = np.arange(len(metrics_labels))

    fig, ax = plt.subplots(figsize=(10, 6))
    input_bars = ax.bar(index - bar_width/2, input_metrics, bar_width, label='Input Image', color='blue')
    generated_bars = ax.bar(index + bar_width/2, generated_metrics, bar_width, label='Generated Image', color='orange')

    ax.set_xlabel('Metrics')
    ax.set_ylabel('Values')
    ax.set_title('Image Quality Metrics Comparison')
    ax.set_xticks(index)
    ax.set_xticklabels(metrics_labels)
    ax.legend()

    def autolabel(bars):
        for bar in bars:
            height = bar.get_height()
            ax.annotate('{}'.format(round(height, 2)),
                        xy=(bar.get_x() + bar.get_width() / 2, height),
                        xytext=(0, 3),
                        textcoords="offset points",
                        ha='center', va='bottom')

    autolabel(input_bars)
    autolabel(generated_bars)

    plt.show()

# Example quality metrics (replace with your calculated metrics)
input_metrics = [75, 120, 0.5]
generated_metrics = [80, 110, 0.4]

# Visualize comparison graph
visualize_quality_comparison(input_metrics, generated_metrics)

"""### **Frchet Inception Distance**"""

! pip install clean-fid

import numpy as np
import cv2
from cleanfid import fid
score_clean = fid.compute_fid("/content/sample_data/input image", "/content/sample_data/generated img", mode="clean", num_workers=0)
print(f"clean-fid score is {score_clean:.3f}")

# expect a value of 76.912
score_tf = fid.compute_fid("/content/sample_data/input image", "/content/sample_data/generated img", mode="legacy_tensorflow", num_workers=0)
print(f"legacy-tensorflow-fid score is {score_tf:.3f}")

"""### **SIMILARITY CHECK METRICS**"""

from skimage.metrics import structural_similarity as compare_ssim
import cv2

def calculate_ssim(input_image, generated_image):
    # Convert images to grayscale
    gray_input = cv2.cvtColor(input_image, cv2.COLOR_RGB2GRAY)
    gray_generated = cv2.cvtColor(generated_image, cv2.COLOR_RGB2GRAY)

    # Compute SSIM
    ssim_score, _ = compare_ssim(gray_input, gray_generated, full=True)
    return ssim_score

# Load input and generated images
input_image_path = "/content/women.jpeg"
generated_image_path = "/content/generated_image.jpg"

input_image = cv2.imread(input_image_path)
input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)

generated_image = cv2.imread(generated_image_path)
generated_image = cv2.cvtColor(generated_image, cv2.COLOR_BGR2RGB)

# Resize input image to match the dimensions of the generated image
generated_height, generated_width, _ = generated_image.shape
input_resized = cv2.resize(input_image, (generated_width, generated_height))

# Calculate SSIM
ssim_score = calculate_ssim(input_resized, generated_image)

print("SSIM Score:", ssim_score)

"""### **CONTENT PRESERVATION SCORE**"""

import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
import cv2

def extract_features(image):
    # Load pre-trained VGG16 model (without the top layers)
    model = VGG16(weights='imagenet', include_top=False)

    # Preprocess image for VGG16 model
    image = cv2.resize(image, (224, 224))  # Resize image to VGG16 input size
    image = np.expand_dims(image, axis=0)  # Add batch dimension
    image = preprocess_input(image)  # Preprocess input for VGG16

    # Extract features using VGG16 model
    features = model.predict(image)

    return features

def calculate_content_preservation(input_image, generated_image):
    # Extract features from input and generated images
    input_features = extract_features(input_image)
    generated_features = extract_features(generated_image)

    # Compute cosine similarity between features
    similarity = np.mean([np.mean(np.dot(input_feature.flatten(), generated_feature.flatten()) /
                                   (np.linalg.norm(input_feature.flatten()) * np.linalg.norm(generated_feature.flatten())))
                          for input_feature, generated_feature in zip(input_features, generated_features)])

    return similarity

# Load input and generated images
input_image_path = "/content/women.jpeg"
generated_image_path = "/content/generated_image.jpg"

input_image = cv2.imread(input_image_path)
input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)

generated_image = cv2.imread(generated_image_path)
generated_image = cv2.cvtColor(generated_image, cv2.COLOR_BGR2RGB)

# Calculate content preservation
content_preservation_score = calculate_content_preservation(input_image, generated_image)

print("Content Preservation Score:", content_preservation_score)

"""### **Color Histogram Comparison**"""

import cv2

def calculate_color_histogram_similarity(input_image, generated_image):
    # Convert images to LAB color space
    input_lab = cv2.cvtColor(input_image, cv2.COLOR_RGB2LAB)
    generated_lab = cv2.cvtColor(generated_image, cv2.COLOR_RGB2LAB)

    # Calculate histograms for LAB channels
    input_hist = [cv2.calcHist([input_lab], [i], None, [256], [0, 256]) for i in range(3)]
    generated_hist = [cv2.calcHist([generated_lab], [i], None, [256], [0, 256]) for i in range(3)]

    # Calculate histogram intersection similarity for each channel
    similarities = [cv2.compareHist(input_hist[i], generated_hist[i], cv2.HISTCMP_INTERSECT) for i in range(3)]

    # Overall similarity is the mean of individual channel similarities
    color_similarity = sum(similarities) / len(similarities)

    return color_similarity

# Load input and generated images
input_image_path = "/content/women.jpeg"
generated_image_path = "/content/generated_image.jpg"

input_image = cv2.imread(input_image_path)
input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)

generated_image = cv2.imread(generated_image_path)
generated_image = cv2.cvtColor(generated_image, cv2.COLOR_BGR2RGB)

# Calculate color histogram similarity
color_similarity_score = calculate_color_histogram_similarity(input_image, generated_image)

print("Color Histogram Similarity Score:", color_similarity_score)















